{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prostate-arizona",
   "metadata": {
    "id": "bA5ajAmk7XH6"
   },
   "source": [
    "# Cleaning Data in Python\n",
    "\n",
    "ðŸ‘‹ Welcome to your workspace! Here, you can write and run Python code and add text in [Markdown](https://www.markdownguide.org/basic-syntax/). Below, we've imported the datasets from the course _Cleaning Data in Python_ as DataFrames as well as the packages used in the course. This is your sandbox environment: analyze the course datasets further, take notes, or experiment with code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleared-simulation",
   "metadata": {
    "executionTime": 4406,
    "lastSuccessfullyExecutedCode": "%%capture\n# Install fuzzywuzzy\n!pip install fuzzywuzzy"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install fuzzywuzzy\n",
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e25fdd8-4d84-45bc-80f0-949917e00a17",
   "metadata": {
    "chartConfig": {
     "bar": {
      "hasRoundedCorners": true,
      "stacked": false
     },
     "type": "bar",
     "version": "v1"
    },
    "executionTime": 3930,
    "lastSuccessfullyExecutedCode": "# Importing course packages; you can add more too!\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport fuzzywuzzy\nimport recordlinkage \n\n# Importing course datasets as DataFrames\nride_sharing = pd.read_csv('datasets/ride_sharing_new.csv', index_col = 'Unnamed: 0')\nairlines = pd.read_csv('datasets/airlines_final.csv',  index_col = 'Unnamed: 0')\nbanking = pd.read_csv('datasets/banking_dirty.csv', index_col = 'Unnamed: 0')\nrestaurants = pd.read_csv('datasets/restaurants_L2.csv', index_col = 'Unnamed: 0')\nrestaurants_new = pd.read_csv('datasets/restaurants_L2_dirty.csv', index_col = 'Unnamed: 0')\n\nride_sharing.head() # Display the first five rows of this DataFrame",
    "visualizeDataframe": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_A_name</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>station_B_name</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_birth_year</th>\n",
       "      <th>user_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 minutes</td>\n",
       "      <td>81</td>\n",
       "      <td>Berry St at 4th St</td>\n",
       "      <td>323</td>\n",
       "      <td>Broadway at Kearny</td>\n",
       "      <td>5480</td>\n",
       "      <td>2</td>\n",
       "      <td>1959</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24 minutes</td>\n",
       "      <td>3</td>\n",
       "      <td>Powell St BART Station (Market St at 4th St)</td>\n",
       "      <td>118</td>\n",
       "      <td>Eureka Valley Recreation Center</td>\n",
       "      <td>5193</td>\n",
       "      <td>2</td>\n",
       "      <td>1965</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8 minutes</td>\n",
       "      <td>67</td>\n",
       "      <td>San Francisco Caltrain Station 2  (Townsend St...</td>\n",
       "      <td>23</td>\n",
       "      <td>The Embarcadero at Steuart St</td>\n",
       "      <td>3652</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 minutes</td>\n",
       "      <td>16</td>\n",
       "      <td>Steuart St at Market St</td>\n",
       "      <td>28</td>\n",
       "      <td>The Embarcadero at Bryant St</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11 minutes</td>\n",
       "      <td>22</td>\n",
       "      <td>Howard St at Beale St</td>\n",
       "      <td>350</td>\n",
       "      <td>8th St at Brannan St</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     duration  station_A_id  \\\n",
       "0  12 minutes            81   \n",
       "1  24 minutes             3   \n",
       "2   8 minutes            67   \n",
       "3   4 minutes            16   \n",
       "4  11 minutes            22   \n",
       "\n",
       "                                      station_A_name  station_B_id  \\\n",
       "0                                 Berry St at 4th St           323   \n",
       "1       Powell St BART Station (Market St at 4th St)           118   \n",
       "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
       "3                            Steuart St at Market St            28   \n",
       "4                              Howard St at Beale St           350   \n",
       "\n",
       "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
       "0               Broadway at Kearny     5480          2             1959   \n",
       "1  Eureka Valley Recreation Center     5193          2             1965   \n",
       "2    The Embarcadero at Steuart St     3652          3             1993   \n",
       "3     The Embarcadero at Bryant St     1883          1             1979   \n",
       "4             8th St at Brannan St     4626          2             1994   \n",
       "\n",
       "  user_gender  \n",
       "0        Male  \n",
       "1        Male  \n",
       "2        Male  \n",
       "3        Male  \n",
       "4        Male  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing course packages; you can add more too!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import fuzzywuzzy\n",
    "#import recordlinkage \n",
    "\n",
    "# Importing course datasets as DataFrames\n",
    "ride_sharing = pd.read_csv('datasets/ride_sharing_new.csv', index_col = 'Unnamed: 0')\n",
    "airlines = pd.read_csv('datasets/airlines_final.csv',  index_col = 'Unnamed: 0')\n",
    "banking = pd.read_csv('datasets/banking_dirty.csv', index_col = 'Unnamed: 0')\n",
    "restaurants = pd.read_csv('datasets/restaurants_L2.csv', index_col = 'Unnamed: 0')\n",
    "restaurants_new = pd.read_csv('datasets/restaurants_L2_dirty.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "ride_sharing.head() # Display the first five rows of this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2b015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3317e25-17e0-4f71-89fb-18d950b5bd85",
   "metadata": {},
   "source": [
    "### Don't know where to start?\n",
    "\n",
    "Try completing these tasks:\n",
    "- For each DataFrame, inspect the data types of each column and, where needed, clean and convert columns into the correct data type. You should also rename any columns to have more descriptive titles.\n",
    "- Identify and remove all the duplicate rows in `ride_sharing`.\n",
    "- Inspect the unique values of all the columns in `airlines` and clean any inconsistencies.\n",
    "- For the `airlines` DataFrame, create a new column called `International` from `dest_region`, where values representing US regions map to `False` and all other regions map to `True`.\n",
    "- The `banking` DataFrame contains out of date ages. Update the `Age` column using today's date and the `birth_date` column.\n",
    "- Clean the `restaurants_new` DataFrame so that it better matches the categories in the `city` and `type` column of the `restaurants` DataFrame. Afterward, given typos in restaurant names, use record linkage to generate possible pairs of rows between `restaurants` and `restaurants_new` using criteria you think is best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0f6b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name     object\n",
       "addr     object\n",
       "city     object\n",
       "phone     int64\n",
       "type     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For each DataFrame, inspect the data types of each column and, where needed, \n",
    "#clean and convert columns into the correct data type. You should also rename any columns to have more descriptive titles.\n",
    "\n",
    "ride_sharing.dtypes\n",
    "airlines.dtypes\n",
    "banking.dtypes\n",
    "restaurants.dtypes\n",
    "restaurants_new.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99dcac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows:  4\n",
      "Shape of the original DataFrame:  (25760, 9)\n",
      "Shape of the cleaned DataFrame:  (25756, 9)\n"
     ]
    }
   ],
   "source": [
    "#Identify and remove all the duplicate rows in `ride_sharing`.\n",
    "\n",
    "# Identifying duplicate rows\n",
    "duplicate_rows = ride_sharing.duplicated()\n",
    "\n",
    "# Counting the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "print(\"Number of duplicate rows: \", num_duplicates)\n",
    "\n",
    "# Dropping duplicate rows\n",
    "ride_sharing_cleaned = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Verifying the result\n",
    "print(\"Shape of the original DataFrame: \", ride_sharing.shape)\n",
    "print(\"Shape of the cleaned DataFrame: \", ride_sharing_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feec4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column 'id':\n",
      "[1351  373 2820 ... 2684 2549 2162]\n",
      "\n",
      "Unique values in column 'day':\n",
      "['Tuesday' 'Friday' 'Thursday' 'Wednesday' 'Saturday' 'Sunday' 'Monday']\n",
      "\n",
      "Unique values in column 'airline':\n",
      "['UNITED INTL' 'ALASKA' 'DELTA' 'SOUTHWEST' 'AMERICAN' 'JETBLUE'\n",
      " 'AEROMEXICO' 'AIR CANADA' 'UNITED' 'INTERJET' 'TURKISH AIRLINES'\n",
      " 'AIR FRANCE/KLM' 'HAWAIIAN AIR' 'COPA' 'WOW' 'KOREAN AIR' 'EMIRATES'\n",
      " 'AVIANCA' 'AER LINGUS' 'CATHAY PACIFIC' 'BRITISH AIRWAYS'\n",
      " 'PHILIPPINE AIRLINES' 'LUFTHANSA' 'QANTAS' 'FRONTIER' 'CHINA EASTERN'\n",
      " 'EVA AIR' 'VIRGIN ATLANTIC' 'AIR NEW ZEALAND' 'SINGAPORE AIRLINES'\n",
      " 'AIR CHINA' 'CHINA SOUTHERN' 'ANA ALL NIPPON']\n",
      "\n",
      "Unique values in column 'destination':\n",
      "['KANSAI' 'SAN JOSE DEL CABO' 'LOS ANGELES' 'MIAMI' 'NEWARK' 'LONG BEACH'\n",
      " 'MEXICO CITY' 'TORONTO' 'PORTLAND' 'SAN DIEGO' 'BOSTON' 'SPOKANE'\n",
      " 'GUADALAJARA' 'MINNEAPOLIS-ST. PAUL' 'NEW YORK-JFK' 'ISTANBUL'\n",
      " 'BALTIMORE' 'LAS VEGAS' 'SHANGHAI' 'TOKYO-NARITA' 'PARIS-DE GAULLE'\n",
      " 'HONOLULU' 'DALLAS-FT. WORTH' 'PANAMA CITY' 'PHOENIX' 'REYKJAVIK'\n",
      " 'SAN ANTONIO' 'HONG KONG' 'SEOUL' 'DUBAI' \"CHICAGO-O'HARE\" 'INDIANAPOLIS'\n",
      " 'SAN SALVADOR' 'SALT LAKE CITY' 'BEIJING' 'DUBLIN' 'WASHINGTON DC-DULLES'\n",
      " 'LONDON HEATHROW' 'MANILA' 'RALEIGH-DURHAM' 'VANCOUVER' 'MUNICH'\n",
      " 'NEW ORLEANS' 'FRANKFURT' 'SYDNEY' 'KAHULUI' 'AMSTERDAM' 'ATLANTA'\n",
      " 'SEATTLE' 'DETROIT' 'SANTA BARBARA' 'PHILADELPHIA' 'DENVER' 'BAKERSFIELD'\n",
      " 'AUSTIN' 'CALGARY' 'TAIPEI' 'ONTARIO (CALIF)' 'BURBANK' 'CHARLOTTE'\n",
      " 'AUCKLAND' 'SINGAPORE' 'ORLANDO' 'NASHVILLE' 'WUHAN' 'HOUSTON-BUSH'\n",
      " 'FT. LAUDERDALE' 'SANTA ANA' 'EUGENE' 'KANSAS CITY' 'QINGDAO'\n",
      " 'PUERTO VALLARTA']\n",
      "\n",
      "Unique values in column 'dest_region':\n",
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east']\n",
      "\n",
      "Unique values in column 'dest_size':\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     ']\n",
      "\n",
      "Unique values in column 'boarding_area':\n",
      "['Gates 91-102' 'Gates 50-59' 'Gates 40-48' 'Gates 20-39' 'Gates 1-12'\n",
      " 'Gates 70-90' 'Gates 60-69']\n",
      "\n",
      "Unique values in column 'dept_time':\n",
      "['2018-12-31' '2018-01-01']\n",
      "\n",
      "Unique values in column 'wait_min':\n",
      "[ 115.  135.   70.  190.  559.  140.   63.  215.  180.  540.  192.  107.\n",
      "  155.  175.  100.  225.   60.  145.  210.  160.  604.  205.  510.  270.\n",
      "  173.   92.  125.  120.  900.  150.  330.  110.  415.   76.  165.  122.\n",
      "   90.   68.   15.   65.  300.   77.  185.   60.   81.  105.  170.  245.\n",
      "  355.   85.  177.  130.   50.   32.  103.  240.  305.   75.  235.   95.\n",
      "  112.  142.   85.  265.   97.  131.  514.   75.   98.  166.   65.  139.\n",
      "  220.   53.  153.  123.  195.  335.  216.  247.   45.  250.  182.  119.\n",
      "  295.   55.  362.  179.  260.  243.   89.   50.   65.   80.  385.  420.\n",
      "  390.  152.  276.  515.  200.   82. 1365.  224.  350.  183.  607.   55.\n",
      "  158.  231.  148.  230.  132.  485.  199.  585.  104.   87.  143.  213.\n",
      "  334.   88.   80.  211.   62.   95.  207.   70.  685.  101.  113.  111.\n",
      "  156.   96.  212.   30.  191.  100.  360.   85.   30.  365.  161.  127.\n",
      "  109.  445.   58.   86.  259.  193.  167.   97.  218.   95.  274.  302.\n",
      "  117.  315.   83.  705.  159.  202.  464.  162.   66.   63.   75.   95.\n",
      "   93.  214.  121.  333.  134.  154.  380.  128.  367.   83.  108.  164.\n",
      "  320.   98.  147.  505.   49.   79.  850.   91.   20.   84.  430.  255.\n",
      "   90.  695.  285.  268.  299. 1210.   80.  124.  118.  289.   90.  475.\n",
      "  600.  419.  126.  271.  400.  217.  286.  157.  450.  340.  280.  138.\n",
      "   92.   70.  275.   25.   74.  930.  455.  136.  176.  116.  209.   71.\n",
      "  151.  181.   64.  263.  325.   89.  102.  341.  610.  184.   72.   78.\n",
      "  273.  137.  370.   20.  395.  375.  838.  172.  133.   73.   85.  246.\n",
      "   60.  985.  644.   57.  342.   50.   72.  405.   15.   88.  227.   40.\n",
      "   87.   56.  292.  425.  169.  345.  282.  168.  725.   94.  100.  650.\n",
      "  595.  640.  219.   67.   62.   35.  187.  863.  163. 1035.   35.   65.\n",
      "  291.   40.  290.   74.  146.  790.  730.  780.  242.  328.  229.  970.\n",
      "  272.  307.  885.  296.   89.   91.   73.  353.  861.   93.   59.  208.\n",
      "  141.  584.   78.  670.  580.  310.  194.  523.  277.  470.  223.  178.\n",
      "  443.   92.]\n",
      "\n",
      "Unique values in column 'cleanliness':\n",
      "['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty']\n",
      "\n",
      "Unique values in column 'safety':\n",
      "['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe']\n",
      "\n",
      "Unique values in column 'satisfaction':\n",
      "['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Inspect the unique values of all the columns in `airlines` and clean any inconsistencies.\n",
    "# Inspect unique values in each column\n",
    "for column in airlines.columns:\n",
    "    unique_values = airlines[column].unique()\n",
    "    print(f\"Unique values in column '{column}':\")\n",
    "    print(unique_values)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c3aba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id        day        airline        destination    dest_region  \\\n",
      "0     1351    Tuesday    UNITED INTL             KANSAI           Asia   \n",
      "1      373     Friday         ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2     2820   Thursday          DELTA        LOS ANGELES        West US   \n",
      "3     1157    Tuesday      SOUTHWEST        LOS ANGELES        West US   \n",
      "4     2992  Wednesday       AMERICAN              MIAMI        East US   \n",
      "...    ...        ...            ...                ...            ...   \n",
      "2804  1475    Tuesday         ALASKA       NEW YORK-JFK        East US   \n",
      "2805  2222   Thursday      SOUTHWEST            PHOENIX        West US   \n",
      "2806  2684     Friday         UNITED            ORLANDO        East US   \n",
      "2807  2549    Tuesday        JETBLUE         LONG BEACH        West US   \n",
      "2808  2162   Saturday  CHINA EASTERN            QINGDAO           Asia   \n",
      "\n",
      "     dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0          Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1        Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2          Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "3          Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
      "4          Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
      "...        ...           ...         ...       ...             ...   \n",
      "2804       Hub   Gates 50-59  2018-12-31     280.0  Somewhat clean   \n",
      "2805       Hub   Gates 20-39  2018-12-31     165.0           Clean   \n",
      "2806       Hub   Gates 70-90  2018-12-31      92.0           Clean   \n",
      "2807     Small    Gates 1-12  2018-12-31      95.0           Clean   \n",
      "2808     Large    Gates 1-12  2018-12-31     220.0           Clean   \n",
      "\n",
      "             safety        satisfaction International  \n",
      "0           Neutral      Very satisfied           NaN  \n",
      "1         Very safe      Very satisfied           NaN  \n",
      "2     Somewhat safe             Neutral           NaN  \n",
      "3         Very safe  Somewhat satsified           NaN  \n",
      "4         Very safe  Somewhat satsified           NaN  \n",
      "...             ...                 ...           ...  \n",
      "2804        Neutral  Somewhat satsified           NaN  \n",
      "2805      Very safe      Very satisfied           NaN  \n",
      "2806      Very safe      Very satisfied           NaN  \n",
      "2807  Somewhat safe      Very satisfied           NaN  \n",
      "2808      Very safe  Somewhat satsified           NaN  \n",
      "\n",
      "[2477 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#For the `airlines` DataFrame, create a new column called `International` from `dest_region`, where values representing US regions map \n",
    "# to `False` and all other regions map to `True`.\n",
    "\n",
    "\n",
    "# Create a mapping dictionary for the International column\n",
    "mapping = {'US': False, 'EU': True, 'AS': True, 'SA': True, 'OC': True, 'AF': True}\n",
    "\n",
    "# Create the International column using the map() function\n",
    "airlines['International'] = airlines['dest_region'].map(mapping)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(airlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eda642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cust_id birth_date   Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "0   870A9281 1962-06-09  60.0     63523.31       51295  30105.0   4138.0   \n",
      "1   166B05B0 1962-12-16  60.0     38175.46       15050   4995.0    938.0   \n",
      "2   BFC13E88 1990-09-12  32.0     59863.77       24567  10323.0   4590.0   \n",
      "3   F2158F66 1985-11-03  37.0     84132.10       23712   3908.0    492.0   \n",
      "4   7A73F334 1990-05-17  33.0    120512.00       93230  12158.4  51281.0   \n",
      "..       ...        ...   ...          ...         ...      ...      ...   \n",
      "95  CA507BA1 1974-08-10  48.0     12209.84        7515    190.0    931.0   \n",
      "96  B99CD662 1989-12-12  33.0     92838.44       49089   2453.0   7892.0   \n",
      "97  13770971 1984-11-29  38.0     92750.87       27962   3352.0   7547.0   \n",
      "98  93E78DA3 1969-12-14  53.0     41942.23       29662   1758.0  11174.0   \n",
      "99  AC91D689 1993-05-18  30.0     99490.61       32149   2184.0  17918.0   \n",
      "\n",
      "     fund_C   fund_D account_opened last_transaction  \n",
      "0    1420.0  15632.0       02-09-18         22-02-19  \n",
      "1    6696.0   2421.0       28-02-19         31-10-18  \n",
      "2    8469.0   1185.0       25-04-18         02-04-18  \n",
      "3    6482.0  12830.0       07-11-17         08-11-18  \n",
      "4   13434.0  18383.0       14-05-18         19-07-18  \n",
      "..      ...      ...            ...              ...  \n",
      "95   1451.0   4943.0       26-05-18         11-09-19  \n",
      "96  31486.0   7258.0       04-05-17         12-03-19  \n",
      "97   8486.0   8577.0       16-08-17         24-04-19  \n",
      "98  11650.0   5080.0       09-10-17         15-04-18  \n",
      "99   6714.0   5333.0       01-08-17         04-08-19  \n",
      "\n",
      "[100 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#The `banking` DataFrame contains out of date ages. Update the `Age` column using today's date and the `birth_date` column.\n",
    "\n",
    "# Convert birth_date column to datetime type\n",
    "banking['birth_date'] = pd.to_datetime(banking['birth_date'])\n",
    "\n",
    "# Calculate the age based on today's date\n",
    "today = pd.to_datetime('today')\n",
    "banking['Age'] = (today - banking['birth_date']).astype('<m8[Y]')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(banking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092024a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/italovega/opt/anaconda3/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential match: restaurants[0] - restaurants_new[40]\n",
      "Potential match: restaurants[1] - restaurants_new[28]\n",
      "Potential match: restaurants[2] - restaurants_new[74]\n",
      "Potential match: restaurants[3] - restaurants_new[1]\n",
      "Potential match: restaurants[4] - restaurants_new[53]\n",
      "Potential match: restaurants[5] - restaurants_new[65]\n",
      "Potential match: restaurants[6] - restaurants_new[73]\n",
      "Potential match: restaurants[7] - restaurants_new[79]\n",
      "Potential match: restaurants[8] - restaurants_new[43]\n",
      "Potential match: restaurants[9] - restaurants_new[50]\n",
      "Potential match: restaurants[10] - restaurants_new[75]\n",
      "Potential match: restaurants[11] - restaurants_new[21]\n",
      "Potential match: restaurants[12] - restaurants_new[26]\n",
      "Potential match: restaurants[13] - restaurants_new[7]\n",
      "Potential match: restaurants[14] - restaurants_new[67]\n",
      "Potential match: restaurants[15] - restaurants_new[55]\n",
      "Potential match: restaurants[16] - restaurants_new[57]\n",
      "Potential match: restaurants[17] - restaurants_new[12]\n",
      "Potential match: restaurants[18] - restaurants_new[71]\n",
      "Potential match: restaurants[19] - restaurants_new[47]\n",
      "Potential match: restaurants[20] - restaurants_new[20]\n",
      "Potential match: restaurants[21] - restaurants_new[27]\n",
      "Potential match: restaurants[80] - restaurants_new[40]\n",
      "Potential match: restaurants[83] - restaurants_new[13]\n"
     ]
    }
   ],
   "source": [
    "#Clean the `restaurants_new` DataFrame so that it better matches the categories in the `city` and `type` column of the\n",
    "# `restaurants` DataFrame. Afterward, given typos in restaurant names, use record linkage to generate possible pairs of \n",
    "#rows between `restaurants` and `restaurants_new` using criteria you think is best.\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Define the matching criteria\n",
    "threshold = 80  # Adjust the threshold as needed\n",
    "\n",
    "# Generate possible matching pairs\n",
    "potential_matches = []\n",
    "for idx, row in restaurants.iterrows():\n",
    "    name = row['name']\n",
    "    city = row['city']\n",
    "    restaurant_matches = restaurants_new[(restaurants_new['name'].apply(lambda x: fuzz.token_set_ratio(x, name)) > threshold) &\n",
    "                                         (restaurants_new['city'].apply(lambda x: fuzz.token_set_ratio(x, city)) > threshold)]\n",
    "    for _, match in restaurant_matches.iterrows():\n",
    "        potential_matches.append((idx, match.name))\n",
    "\n",
    "# Display the potential matching pairs\n",
    "for pair in potential_matches:\n",
    "    print(f\"Potential match: restaurants[{pair[0]}] - restaurants_new[{pair[1]}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
