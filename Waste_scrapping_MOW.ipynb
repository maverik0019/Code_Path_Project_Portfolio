{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto se originó a partir del trabajo efectuado para la startup 'Market of Waste', donde se necesitaba una amplia base de datos sobre diversos materiales presentes en Chile, detallando sus precios y valorización económica. Dicha base de datos se ha convertido en una herramienta clave para el análisis y comparación con múltiples iniciativas de 'Upcycling'. Para la compilación de estos datos, se utilizó una base de datos específica del proyecto, enriquecida mediante técnicas avanzadas de 'Web Scraping' y el uso de SerpApi. En todo momento, se mantuvo un firme compromiso con el respeto a las políticas de confidencialidad de las empresas y clientes implicados, garantizando así la integridad y privacidad de la información recabada.\n",
    "\n",
    "//\n",
    "\n",
    "This project originated from the work carried out for the startup 'Market of Waste', where a comprehensive database of various materials in Chile was required, including their pricing and economic valuation. This database has become a key tool for analysis and comparison with multiple 'Upcycling' initiatives. For the compilation of this data, a project-specific database was used, enhanced through advanced 'Web Scraping' techniques and the use of SerpApi. Throughout the process, a strong commitment was maintained to respect the confidentiality policies of the involved companies and clients, thus ensuring the integrity and privacy of the collected information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Market of Waste: Tus residuos son un tesoro](https://i.ytimg.com/vi/Rccz2DkfWpc/mqdefault.jpg)](https://www.youtube.com/watch?v=Rccz2DkfWpc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from google.oauth2 import service_account\n",
    "import uuid\n",
    "from serpapi import GoogleSearch\n",
    "import pyarrow as pa\n",
    "\n",
    "#Solo una propuesta a adaptar segun las necesidades de busqueda\n",
    "#Only a proposal to adapt according to the search need\n",
    "\"\"\"\n",
    "ateriales = [\n",
    "    \"Orgánica\",\n",
    "    \"Strech film\", \n",
    "    \"Polietileno de baja transparente\",\n",
    "    \"Polietileno de baja para lavado\",\n",
    "    \"Techo invernadero (no degradado y sin impurezas)\",\n",
    "    \"Botellas Pet blanca\",\n",
    "    \"Botellas Pet color\",\n",
    "    \"PAPELES Y CARTONES\",\n",
    "    \"Papel de oficina (Blanco 2)\",\n",
    "    \"Mixto corriente\",\n",
    "    \"Duplex\",\n",
    "    \"Carton corrugado\",\n",
    "    \"Diario\",\n",
    "    \"VIDRIO\",\n",
    "    \"Vidrio de botellas\",\n",
    "    \"Vidrio de frascos\",\n",
    "    \"CHATARRA\",\n",
    "    \"Fierro chatarra\",\n",
    "    \"Lata chatarra\",\n",
    "    \"Aluminio de latas bebidas\",\n",
    "    \"Aluminio\",\n",
    "    \"Cobre\",\n",
    "    \"Papeles y Cartones\",\n",
    "    \"Plastico\",\n",
    "    \"Vidrio\",\n",
    "    \"Metales\",\n",
    "    \"Textil\",\n",
    "    \"CARTON\",\n",
    "    \"ALUMINIO\",  # Puedes agregar más especificidad como 'latas aplastadas', 'jugo', 'cerveza'\n",
    "    \"METALES\",  # Agrega especificidad si es necesario, como 'tarros de conserva', 'aerosol spray', 'chatarra'\n",
    "    \"PAPEL TIPO 1\",\n",
    "    \"PAPEL TIPO 2\",\n",
    "    \"PAPEL TIPO 3\",\n",
    "    \"PLASTICOS PET-1\",  # Añade 'botellas plásticas desechables -sin tapa' para especificar\n",
    "    \"PLASTICOS PET-1 COLOR\",  # Añade 'botellas plásticas desechables -sin tapa' para especificar\n",
    "    \"PLASTICOS PEAD-2\",  # Añade 'shampoo', 'bolsas', 'juguetes' para especificar\n",
    "    \"PLASTICOS PP-5\",  # Añade 'tapas de botellas', 'bolsas de alimentos y ropa' para especificar\n",
    "    \"TETRA PAK\",\n",
    "    \"VIDRIOS blanco\"  # Añade 'botellas' para especificar\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "materiales = [\"reciclaje\"]  \n",
    "precios = [\"precio\"]\n",
    "search_terms = [f\"{material} {precio}\" for material in materiales for precio in precios]\n",
    "\n",
    "search_location = \"Chile\"\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "for search_term in search_terms:  \n",
    "    for num in range(40):  \n",
    "        start = num * 10\n",
    "        params = {\n",
    "            \"api_key\": \"xxxxxxxxxxxxxxxxxxxxxxx\",\n",
    "            \"device\": \"desktop\",\n",
    "            \"engine\": \"google\",\n",
    "            \"google_domain\": \"google.cl\",\n",
    "            \"q\": search_term,\n",
    "            \"hl\": \"es\",\n",
    "            \"gl\": \"cl\",\n",
    "            \"location\": search_location,\n",
    "            \"start\": start,\n",
    "        }\n",
    "\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "\n",
    "       \n",
    "        current_page_results = pd.DataFrame(results.get('organic_results', []))\n",
    "        all_results = pd.concat([all_results, current_page_results], ignore_index=True)\n",
    "\n",
    "all_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, utilizamos Google Cloud Platform (GCP) para almacenar los dataframes generados, aprovechando la robustez y capacidad de Google Cloud Storage. Elegimos Google BigQuery como nuestra principal herramienta de análisis debido a su eficiencia en el manejo de grandes volúmenes de datos. Esta integración se realizó sin problemas, facilitada por un convenio entre la startup y Google, asegurando una gestión eficiente y segura de los datos en la nube.\n",
    "\n",
    "//\n",
    "\n",
    "In this project, we utilized Google Cloud Platform (GCP) for storing the generated dataframes, harnessing the robustness and capacity of Google Cloud Storage. We chose Google BigQuery as our main analysis tool, given its efficiency in handling large data volumes. This integration was seamlessly facilitated by an existing agreement between the startup and Google, ensuring efficient and secure data management in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo JSON credencial//Path to credentials JSON file\n",
    "key_path = \"your_Path/real/file.json\"\n",
    "\n",
    "# ID proyecto Google Cloud// Google Cloud project ID/\n",
    "project_id = \"MOW\"\n",
    "\n",
    "# ID DadaFrame & nombre tabla en BigQuery//# DadaFrame ID & table name in BigQuery\n",
    "dataset_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "table_name = \"xxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# ID completo de la tabla//Full ID of the table\n",
    "table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "# Cargar credencial(JSON file)//Load credential(JSON file)//\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "# Creacion cliente de BigQuery//# BigQuery client creation\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Insertar resultados en tabla de BigQuery//# Insert results into BigQuery table\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "job = client.load_table_from_dataframe(all_results, table_id, job_config=job_config)\n",
    "job.result() \n",
    "\n",
    "print(f\"Data loaded into table {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful storage of the data, we initiated a meticulous filtration process on the generated dataframe. This was done to align it with the specific requirements and anticipated applications of the project. The filtration involved an in-depth extraction of targeted information from a range of websites, ensuring the data was precisely curated to meet the project's objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://reciclaje123.cl\",\n",
    "        #\"https://xxxxxxxxxxxxxx.cl\",\n",
    "        #................\n",
    "        ]\n",
    "\n",
    "datos = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        respuesta = requests.get(url)\n",
    "        contenido_web = respuesta.content\n",
    "        soup = BeautifulSoup(contenido_web, 'html.parser')\n",
    "        tabla = soup.find('table', {'class': 'tableizer-table'})\n",
    "\n",
    "        if tabla:\n",
    "            for fila in tabla.find_all('tr')[1:]:\n",
    "                columnas = fila.find_all('td')\n",
    "                if len(columnas) == 2:\n",
    "                    material = columnas[0].text.strip()\n",
    "                    precio = columnas[1].text.strip()\n",
    "                    datos.append((url, material, precio))\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {url}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(datos, columns=['URL', 'Material', 'Precio'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
